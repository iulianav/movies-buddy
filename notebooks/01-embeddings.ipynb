{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0",
   "metadata": {},
   "source": [
    "# How Can We Represent Semantics? Embeddings 101"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1",
   "metadata": {},
   "source": [
    "Since high school computer programming classes, everyone knows that computers can only understand numbers. So, how can Large Language Models (LLMs) such as ChatGPT understand humans language?\n",
    "\n",
    "## A Bit of History\n",
    "\n",
    "A naive way to do this, is to **map every word in a dictionary to a number**. That is, map a sentence like \"Applied Machine Learning Days are a super cool conference\" to a list of integers like the following:\n",
    "\n",
    "```\n",
    "[10, 40, 5, 10, 7, 8, 90, 123, 2]\n",
    "```\n",
    "\n",
    "This process is called **encoding**, and this is a very simple encoding method. However, for complex Natural Language Processing (NLP) tasks, this is not sufficient. For example, here the word `cool` is used to denote something that is excellent, admirable. But `cool` can also mean \"almost cold\", or even \"relaxed\": how can we disambiguate?\n",
    "\n",
    "To make another, slightly more complex example, take the two pair of words (\"man\", \"woman\") and (\"king\", \"queen\"). We know there is a relationship between those concepts: paraphrasing the words of a [very famous research paper](https://arxiv.org/abs/1301.3781), we might want our encoding method to have this property: encoding(\"King\") - encoding(\"Man\") + encoding(\"Woman\") ~= encoding(\"Queen\").\n",
    "\n",
    "In other words, we would want our encoding method to carry the semantic relationship between words. In 2013, the four authors of the paper came up with Word2Vec: a neural network capable of generating dense vector representations, called *embeddings*, from words, capturing a significant amount of language semantic. You can imagine that the encoding perform this mapping:\n",
    "\n",
    "```\n",
    "\"Queen\" = [0.3, 0.3, 0.2, ..., 0.3]\n",
    "\"King\" = [0.5, -0.3, 0.1, ..., 0.5]\n",
    "\"Man\" = [0.2, 0.95, 0.3, ..., 0.1]\n",
    "\"Woman\" = [0.56, -0.5, 0.32, ..., 0.1]\n",
    "```\n",
    "\n",
    "Where the number of dimensions of each vector is the same for every word (which is yet another desireable feature from our encoding algorithm) and is used to \"place\" every word in an Euclidean space. During training, such neural networks adjust the position of every word in the vector space, and produce a representation that carries the semantic proximity of the terms of the data it was trained on.\n",
    "\n",
    "Nowadays, such models have evolved beyond embedding words alone. Now, for example, it is possible to encode sentences altogether (and this is what we'll do!), as well as images, text and images, as well as audio tracks. But talk is cheap! Let's get our hands dirty, and make our computer understand language!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2",
   "metadata": {},
   "source": [
    "# Your First Vector Embedding"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3",
   "metadata": {},
   "source": [
    "As we discussed early, we need to help our laptop  understand sentences. We talk about embeddings models, it seems a scary concept and it could be if we deal with all the details. Luckily someone did a lot of works for us, we can just donwload pre-trained sentence embedings model from HuggingFace repository (this might take a bit):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sentence_transformers import SentenceTransformer\n",
    "\n",
    "encoder = SentenceTransformer(\"sentence-transformers/all-MiniLM-L6-v2\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5",
   "metadata": {},
   "source": [
    "Encoding is straighforward:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6",
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding_example = encoder.encode(\"My first encoded sentence: Hello, AMLD!\")\n",
    "\n",
    "print(\n",
    "    f\"First and last part of the resulting vector:\\n{embedding_example[:5]} ... {embedding_example[-5:]}\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7",
   "metadata": {},
   "source": [
    "Congratulations! You have encoded your first sentence! How many dimensions does this vector have?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8",
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding_example.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9",
   "metadata": {},
   "source": [
    "If you changed example, you'd notice that the resulting encoded vector would still have the same number of dimensions.\n",
    "\n",
    "This is by far the most thrilling result. To appreciate the representational power of embeddings, we need to move a step forward and embed a bunch of sentences. How about we put it to the test with a more meaninfgul example?\n",
    "\n",
    "Here is a utility function to load a dataset of sentences about different fields, such as \"Nature and Enviroment\" and \"Sports\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10",
   "metadata": {},
   "outputs": [],
   "source": [
    "from movie_buddy.data import get_sentences_dataset\n",
    "\n",
    "sentences = get_sentences_dataset()\n",
    "sentences"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11",
   "metadata": {},
   "source": [
    "Now, your turn: how do you encode the sentences in this dataset? Keep in mind, our encoder can encode any sentence in a sequence."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12",
   "metadata": {},
   "outputs": [],
   "source": [
    "sentence_embeddings = encoder.encode(sentences[\"sentences\"].to_list())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13",
   "metadata": {},
   "source": [
    "What's the shape of this dataset?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14",
   "metadata": {},
   "outputs": [],
   "source": [
    "sentence_embeddings.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15",
   "metadata": {},
   "source": [
    "As humans, it's impossible for us to make sense of this many dimensions. We resort to some black magic (AKA [UMAP](https://umap-learn.readthedocs.io/en/latest/index.html)) to sharply reduce the number of dimensions to 2.\n",
    "\n",
    "This greatly diminshes the representational power of our embeddings, but will leave just enough to surprise us and still be interpretable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16",
   "metadata": {},
   "outputs": [],
   "source": [
    "from movie_buddy.utils import reduce_dimensions, add_umap_to_dataset\n",
    "\n",
    "sentence_embeddings_reduced = reduce_dimensions(sentence_embeddings)\n",
    "sentences_and_embeddings = add_umap_to_dataset(sentences, sentence_embeddings_reduced)\n",
    "\n",
    "sentences_and_embeddings.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17",
   "metadata": {},
   "outputs": [],
   "source": [
    "from movie_buddy.utils import plot_sentences\n",
    "\n",
    "plot_sentences(sentences_and_embeddings)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18",
   "metadata": {},
   "source": [
    "Note that, encoder didn't know nothing about the field of the sentence, could you see some interesting pattern?\n",
    "\n",
    "Now, it is your turn! Add more sentences and try to guess in which zone of the plot will be positioned."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19",
   "metadata": {},
   "outputs": [],
   "source": [
    "your_sentences = [\n",
    "    \"Schools are very important for our society\",\n",
    "    \"I run every day\",\n",
    "    \"AI will revoluzionize the computer science industry\",\n",
    "    # PUT YOUR SENTENCES DOWN THERE IN THE LIST\n",
    "    # ...\n",
    "    # ...\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20",
   "metadata": {},
   "outputs": [],
   "source": [
    "from movie_buddy.utils import add_sentences\n",
    "\n",
    "(\n",
    "    sentences_and_embeddings.pipe(\n",
    "        add_sentences, sentences=your_sentences, encoder=encoder\n",
    "    ).pipe(plot_sentences)\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21",
   "metadata": {},
   "source": [
    "## What About Movies? "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22",
   "metadata": {},
   "source": [
    "At the end of the day we want to build an AI movies assistant, so what about movies? \n",
    "\n",
    "We have a dataset containing some information such as title, overview, genre, release date about ~42000 movies. We can try to embed overviews and try to see if the encoder find some structure inside it. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23",
   "metadata": {},
   "outputs": [],
   "source": [
    "from movie_buddy.data import get_movies_dataset\n",
    "\n",
    "movies = get_movies_dataset()\n",
    "\n",
    "movies.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24",
   "metadata": {},
   "outputs": [],
   "source": [
    "movies_embeddings = encoder.encode(movies[\"overview\"].to_list())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25",
   "metadata": {},
   "outputs": [],
   "source": [
    "from movie_buddy.utils import plot_movies\n",
    "\n",
    "(\n",
    "    movies.pipe(add_umap_to_dataset, reduce_dimensions(movies_embeddings)).pipe(\n",
    "        plot_movies\n",
    "    )\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
