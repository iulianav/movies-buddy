{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0",
   "metadata": {},
   "source": [
    "# Vector Databases"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1",
   "metadata": {},
   "source": [
    "In the previous section of this workshop, we discussed the importance of embedding a language's semantics in vectors (typically with hundreds of dimensions). Modern AI applications such as RAG systems or search engines need to access vast amounts of vectors quickly.\n",
    "\n",
    "In this part, we will delve into vector databases, a technology that permits indexing, storing, and accessing millions of vectors. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2",
   "metadata": {},
   "source": [
    "## We need some dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from scipy.spatial.distance import cityblock, euclidean, cosine\n",
    "from qdrant_client import models, QdrantClient\n",
    "\n",
    "import time\n",
    "\n",
    "from movie_buddy.preprocessing.movies_dataset import get_movies_dataset\n",
    "from sentence_transformers import SentenceTransformer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4",
   "metadata": {},
   "source": [
    "## What Does It Mean That Vectors Are Similar?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5",
   "metadata": {},
   "source": [
    "One of the main advantages of having text represented as a vector is that we can do some mathematics on it! Using different distance metrics, we can calculate how much two points in a multidimensional space are close to each other. The most commonly used in the context of NLP are: \n",
    "\n",
    "- Cosine Similarity\n",
    "- Dot Product\n",
    "- Euclidean Distance\n",
    "- Manhattan Distance"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6",
   "metadata": {},
   "source": [
    "## Are You a Math Geek? Let's see some formulas\n",
    "\n",
    "If you are more interested in distance metrics mathematics, this paragraph is made for you. Otherwise, it is not necessary to understand the rest of the workshop; skip it!\n",
    "\n",
    "### Distance Metrics Property\n",
    "Do you want to invent new distance metrics? You can, but you must have some mathematical properties. So, given two vectors $a$ and $b$, a distance metric $d$ must be: \n",
    "1. **Non-negative**: $d(a, b) >= 0$;\n",
    "\n",
    "2. **Symmetric**: $d(a, b) = d(a, b)$;\n",
    "\n",
    "3. **Respect triangle inequality**: $d(a, b) <= d(a, r) + d(r, b)$ for all vectors $a$, $b$, $r$;\n",
    "\n",
    "4. $d(p, q)=0$ only if $p=q$.\n",
    "\n",
    "Okay, now let's see some distance formulas,: \n",
    "\n",
    "\n",
    "### Cosine Similarity\n",
    "$$\n",
    "d(a, b) = \\cos(\\theta) = \\frac{\\sum_{i=1}^{n} a_i b_i}{\\sqrt{\\sum_{i=1}^{n} a_i^2} \\cdot \\sqrt{\\sum_{i=1}^{n} b_i^2}}\n",
    "$$\n",
    "### Dot Product\n",
    "$$\n",
    "d(a, b) = a \\cdot b = \\sum_{i=1}^{n} a_i b_i\n",
    "$$\n",
    "\n",
    "### Euclidean Distance\n",
    "$$\n",
    "d\\left( a,b\\right)   = \\sqrt {\\sum _{i=1}^{n}  \\left( a_{i}-b_{i}\\right)^2 } \n",
    "$$\n",
    "### Manhattan Distance\n",
    "$$\n",
    "d(a, b) = \\sum_{i=1}^{n} |a_i - b_i|\n",
    "$$\n",
    "\n",
    "**It is your turn!** In the following cell, try changing vector_a and vector_b to see how the different distances differ between the same vector!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7",
   "metadata": {},
   "outputs": [],
   "source": [
    "vector_a = np.array([0.9, 0.1, 0.23, 0.15])\n",
    "vector_b = np.array([0.9, 0.30, 0.23, 0.25])\n",
    "\n",
    "cosine_distance = cosine(vector_a, vector_b)\n",
    "dot_distance = np.dot(vector_a, vector_b)\n",
    "euclidean_dist = euclidean(vector_a, vector_b)\n",
    "manhattan_dist = cityblock(vector_a, vector_b)\n",
    "\n",
    "print(\n",
    "    f\"Cosine: {cosine_distance}\\nDot: {dot_distance}\\nEuclidean: {euclidean_dist}\\nManhattan: {manhattan_dist}\\n\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8",
   "metadata": {},
   "source": [
    "Try to guess in which case one metric is better than another. Which metric I would choose for which type of problem? "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9",
   "metadata": {},
   "source": [
    "## Why We Need Vector Databases? "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10",
   "metadata": {},
   "source": [
    "Can't we just choose a distance and get the k-closest vectors with a classical **K-Nearest-Neighbour** algorithm? \n",
    "\n",
    "Yes, we can! Let's try it. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11",
   "metadata": {},
   "outputs": [],
   "source": [
    "def k_closest_vectors(target_vec, vectors, k):\n",
    "    distances = np.sqrt(\n",
    "        np.sum((vectors - target_vec) ** 2, axis=1)\n",
    "    )  # Yes, you can implement euclidean distance by yourself.\n",
    "    k_closest_indices = np.argsort(distances)[:k]\n",
    "    closest_vectors = vectors[k_closest_indices]\n",
    "    return closest_vectors"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12",
   "metadata": {},
   "source": [
    "Let's forget for a moment of real sentences and we generate a random vector to search..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13",
   "metadata": {},
   "outputs": [],
   "source": [
    "VECTOR_DIM = 500\n",
    "vector_to_search = np.random.uniform(low=-1, high=1, size=(VECTOR_DIM,))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14",
   "metadata": {},
   "source": [
    "... and a list of vector in which search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15",
   "metadata": {},
   "outputs": [],
   "source": [
    "N_VECTORS = 500000\n",
    "K = 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16",
   "metadata": {},
   "outputs": [],
   "source": [
    "vectors = np.random.uniform(low=0.0, high=1, size=(N_VECTORS, VECTOR_DIM))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17",
   "metadata": {},
   "outputs": [],
   "source": [
    "s_time = time.time()\n",
    "closest_vectors = k_closest_vectors(vector_to_search, vectors, K)\n",
    "e_time = time.time()\n",
    "nn_total_time = e_time - s_time\n",
    "\n",
    "f\"To get the closer {K} vectors with naive K-Nearest-Neighbour algorithm took {nn_total_time:.2} sec\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18",
   "metadata": {},
   "source": [
    "Try increasing the N_VECTORS parameter. Did you see the problem? K-Nearest-Neighbour is a $O(N)$ algorithm that scales poorly with many vectors!\n",
    "\n",
    "Instead, Vector Databases use efficient algorithms and data structures to store and search vectors. Of course, these algorithms are not some magic from a wizard. The general idea is to sacrifice some accuracy for a rapid vector search. This approach is called Approximate Nearest-Neighbour (ANN). In the year, researchers and engineers developed many ANN algorithms; the most popular nowadays are:\n",
    "\n",
    "- HNSW: Hierarchical Navigable Small Worlds\n",
    "    - Type: graph-based\n",
    "    - [How it works?](https://towardsdatascience.com/similarity-search-part-4-hierarchical-navigable-small-world-hnsw-2aad4fe87d37)\n",
    "    - original paper [here](https://arxiv.org/abs/1603.09320)\n",
    "- ANNOY: Approximate Nearest Neighbour (Oh Yeah)\n",
    "    - Type: tree-based\n",
    "    - [How it works?](https://erikbern.com/2015/10/01/nearest-neighbors-and-vector-models-part-2-how-to-search-in-high-dimensional-spaces.html)\n",
    "    - Curious about code? repo [here](https://github.com/spotify/annoy)\n",
    "- LSH: Locality Sensitive Hashing\n",
    "    - Type: hash-based\n",
    "    - [How it works?](https://www.notion.so/Vector-Database-101-1a1926611dc548c1a01a8f939a9dc42c?pvs=21)\n",
    "\n",
    "We will use [Qdrant](https://qdrant.tech/) to build our RAG. Let's try to load our random vectors inside a qdrant collection."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19",
   "metadata": {},
   "outputs": [],
   "source": [
    "qdrant = QdrantClient(\":memory:\")\n",
    "\n",
    "COLLECTION_NAME = \"random_vectors\"\n",
    "DISTANCE = models.Distance.COSINE\n",
    "\n",
    "qdrant.recreate_collection(\n",
    "    collection_name=COLLECTION_NAME,\n",
    "    vectors_config=models.VectorParams(size=VECTOR_DIM, distance=DISTANCE),\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20",
   "metadata": {},
   "outputs": [],
   "source": [
    "qdrant.upload_collection(collection_name=COLLECTION_NAME, vectors=vectors)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21",
   "metadata": {},
   "outputs": [],
   "source": [
    "s_time = time.time()\n",
    "qdrant.search(collection_name=COLLECTION_NAME, query_vector=vector_to_search, limit=K)\n",
    "e_time = time.time()\n",
    "ann_total_time = e_time - s_time\n",
    "f\"To get the closer {K} vectors with a vector database took {ann_total_time:.2} sec\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "23",
   "metadata": {},
   "source": [
    "##Â What About Movies?\n",
    "\n",
    "You should now be and expert on vector database, qdrant and embeddings, try yourself to import our movies dataset overviews in qdrant:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: REMOVE CODE THAT PARTECIPANTS SHOULD write"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25",
   "metadata": {},
   "outputs": [],
   "source": [
    "movies_df = get_movies_dataset()\n",
    "movies_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26",
   "metadata": {},
   "source": [
    "Now we don't want to work with random data anymore let's instanciate our encoder...\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27",
   "metadata": {},
   "outputs": [],
   "source": [
    "encoder = SentenceTransformer(\"sentence-transformers/all-MiniLM-L6-v2\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28",
   "metadata": {},
   "source": [
    ".. and embed our movies overviews.."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29",
   "metadata": {},
   "outputs": [],
   "source": [
    "# movies = encoder.encode(movies_df[\"overview\"].tolist())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30",
   "metadata": {},
   "source": [
    "### Build the Collection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31",
   "metadata": {},
   "outputs": [],
   "source": [
    "qdrant = QdrantClient(\":memory:\")\n",
    "\n",
    "COLLECTION_NAME = \"movies\"\n",
    "\n",
    "qdrant.recreate_collection(\n",
    "    collection_name=COLLECTION_NAME,\n",
    "    vectors_config=models.VectorParams(\n",
    "        size=encoder.get_sentence_embedding_dimension(), distance=models.Distance.COSINE\n",
    "    ),\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32",
   "metadata": {},
   "outputs": [],
   "source": [
    "records = [\n",
    "    models.Record(\n",
    "        id=idx,\n",
    "        vector=encoder.encode(r[\"overview\"]).tolist(),\n",
    "        payload={\n",
    "            \"title\": r[\"title\"],\n",
    "            \"overview\": r[\"overview\"],\n",
    "            \"release_date\": r[\"release_date\"],\n",
    "            \"runtime\": r[\"runtime\"],\n",
    "            \"genre\": r[\"genre\"],\n",
    "        },\n",
    "    )\n",
    "    for idx, r in movies_df.iterrows()\n",
    "]\n",
    "\n",
    "qdrant.upload_points(collection_name=COLLECTION_NAME, points=records)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33",
   "metadata": {},
   "source": [
    "Now, you could have fun searching your prefered movies!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34",
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = \"Crazy Scientist save the planet from aliens\"\n",
    "encoded_prompt = encoder.encode(prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35",
   "metadata": {},
   "outputs": [],
   "source": [
    "hits = qdrant.search(\n",
    "    collection_name=COLLECTION_NAME, query_vector=encoded_prompt.tolist(), limit=10\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36",
   "metadata": {},
   "outputs": [],
   "source": [
    "for res in hits:\n",
    "    payload = res.payload\n",
    "    print(\n",
    "        \"Title: {title}\\nRelease date: {release_date}\\nRuntime: {runtime}\\n\\n\".format(\n",
    "            title=payload[\"title\"],\n",
    "            release_date=payload[\"release_date\"],\n",
    "            runtime=payload[\"runtime\"],\n",
    "        )\n",
    "    )"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
